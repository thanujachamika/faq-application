{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "E0acWkfLAIv5",
        "outputId": "f3ffaca6-b25b-4edc-fef4-89bb041484be"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "question = df['Question']\n",
        "answer = df['Answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bauZhEsrA_Bp"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = model.encode(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from FlagEmbedding import FlagReranker\n",
        "\n",
        "reranker = FlagReranker('BAAI/bge-reranker-v2-m3', use_fp16=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Program Files\\Python38\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "tokenizerId = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-id-en\")\n",
        "modelId = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-id-en\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizerEnId = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-id\")\n",
        "modelEnId = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-id\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Program Files\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dao Nam\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-inc-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "c:\\Program Files\\Python38\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ],
      "source": [
        "tokenizerInc = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-inc-en\")\n",
        "modelInc = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-inc-en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizerEnInc = AutoTokenizer.from_pretrained(\"thilina/mt5-sinhalese-english\")\n",
        "modelEnInc = AutoModelForSeq2SeqLM.from_pretrained(\"thilina/mt5-sinhalese-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Program Files\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dao Nam\\.cache\\huggingface\\hub\\models--VietAI--envit5-translation. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizerVi = AutoTokenizer.from_pretrained(\"VietAI/envit5-translation\")\n",
        "modelVi = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/envit5-translation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids = tokenizerEnInc(\"How are you\", return_tensors=\"pt\").input_ids\n",
        "outputs = modelEnInc.generate(input_ids=input_ids)\n",
        "translatedSentence = tokenizerEnInc.batch_decode(outputs, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translateToEng(sentence, originLanguage):\n",
        "    if (originLanguage == 'English'):\n",
        "        return sentence\n",
        "    elif (originLanguage == 'Vietnamese'):\n",
        "        input_ids = tokenizerVi(sentence, return_tensors=\"pt\",padding=True).input_ids\n",
        "        outputs = modelVi.generate(input_ids=input_ids, max_length=512)\n",
        "        translatedSentence = tokenizerVi.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return translatedSentence[0][4:]\n",
        "    elif (originLanguage == 'Indonesian'):\n",
        "        input_ids = tokenizerId(sentence, return_tensors=\"pt\",padding=True).input_ids\n",
        "        outputs = modelId.generate(input_ids=input_ids, max_length=512)\n",
        "        translatedSentence = tokenizerId.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return translatedSentence[0]\n",
        "    elif (originLanguage == 'Sinhala'):\n",
        "        input_ids = tokenizerInc(sentence, return_tensors=\"pt\",padding=True).input_ids\n",
        "        outputs = modelInc.generate(input_ids=input_ids, max_length=512)\n",
        "        translatedSentence = tokenizerInc.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return translatedSentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translateFromEng(sentence, originLanguage):\n",
        "    if (originLanguage == 'English'):\n",
        "        return sentence\n",
        "    elif (originLanguage == 'Vietnamese'):\n",
        "        input_ids = tokenizerVi(sentence, return_tensors=\"pt\",padding=True).input_ids\n",
        "        outputs = modelVi.generate(input_ids=input_ids, max_length=512)\n",
        "        translatedSentence = tokenizerVi.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return translatedSentence[0][4:]\n",
        "    elif (originLanguage == 'Indonesian'):\n",
        "        input_ids = tokenizerEnId(sentence, return_tensors=\"pt\",padding=True).input_ids\n",
        "        outputs = modelEnId.generate(input_ids=input_ids, max_length=512)\n",
        "        translatedSentence = tokenizerEnId.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return translatedSentence[0]\n",
        "    elif (originLanguage == 'Sinhala'):\n",
        "        input_ids = tokenizerEnInc(sentence, return_tensors=\"pt\",padding=True).input_ids\n",
        "        outputs = modelEnInc.generate(input_ids=input_ids, max_length=512)\n",
        "        translatedSentence = tokenizerEnInc.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return translatedSentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "def return_answer(user_question, qLanguage, aLanguage):\n",
        "    user_question = translateToEng(user_question, qLanguage)\n",
        "\n",
        "    q_embedding = model.encode(user_question)\n",
        "\n",
        "    cos_sim = util.cos_sim(q_embedding, embeddings)\n",
        "\n",
        "    val = []\n",
        "\n",
        "    for i in range(len(cos_sim[0])):\n",
        "        val.append(cos_sim[0][i].item())\n",
        "\n",
        "    p_answer = answer.copy()\n",
        "    p_question = question.copy()\n",
        "\n",
        "    for i in range(len(val)):\n",
        "        for j in range(i + 1, len(val)):\n",
        "            if (val[i] < val[j]):\n",
        "                temp = val[i]\n",
        "                val[i] = val[j]\n",
        "                val[j] = temp\n",
        "\n",
        "                temp = p_answer[i]\n",
        "                p_answer[i] = p_answer[j]\n",
        "                p_answer[j] = temp\n",
        "                temp = p_question[i]\n",
        "                p_question[i] = p_question[j]\n",
        "                p_question[j] = temp\n",
        "\n",
        "    answer1 = \"Sorry we don't know the answer\"\n",
        "    answer2 = \"\"\n",
        "    answer3 = \"\"\n",
        "    if (reranker.compute_score([p_question[0], user_question]) >= -6):\n",
        "        answer1 = translateFromEng(p_answer[0], aLanguage)\n",
        "    if (reranker.compute_score([p_question[1], user_question]) >= -6):\n",
        "        answer2 = translateFromEng(p_answer[1], aLanguage)\n",
        "    if (reranker.compute_score([p_question[2], user_question]) >= -6):\n",
        "        answer3 = translateFromEng(p_answer[2], aLanguage)\n",
        "\n",
        "    return (answer1, answer2, answer3)\n",
        "\n",
        "translateFromEng(\"Hello\",\"Sinhala\");\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py:977: UserWarning: Expected 2 arguments for function <function return_answer at 0x0000025F1A4170D0>, received 3.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py:985: UserWarning: Expected maximum 2 arguments for function <function return_answer at 0x0000025F1A4170D0>, received 3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7877\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1887, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1472, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "TypeError: return_answer() takes 2 positional arguments but 3 were given\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1887, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1472, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "TypeError: return_answer() takes 2 positional arguments but 3 were given\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1887, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1472, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "TypeError: return_answer() takes 2 positional arguments but 3 were given\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1887, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1472, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "TypeError: return_answer() takes 2 positional arguments but 3 were given\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1887, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1472, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "TypeError: return_answer() takes 2 positional arguments but 3 were given\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1887, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1472, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "TypeError: return_answer() takes 2 positional arguments but 3 were given\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\queueing.py\", line 527, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1887, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\blocks.py\", line 1472, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"C:\\Users\\Dao Nam\\AppData\\Roaming\\Python\\Python38\\site-packages\\gradio\\utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "TypeError: return_answer() takes 2 positional arguments but 3 were given\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr # type: ignore\n",
        "    \n",
        "demo = gr.Interface(\n",
        "    fn=return_answer,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Question\"),\n",
        "        gr.Dropdown(\n",
        "            [\"English\", \"Indonesian\", \"Vietnamese\", \"Sinhala\"], label=\"Question language\", value=\"English\"\n",
        "        ),\n",
        "        gr.Dropdown(\n",
        "            [\"English\", \"Indonesian\", \"Vietnamese\", \"Sinhala\"], label=\"Answer language\", value=\"English\"\n",
        "        )\n",
        "    ],\n",
        "    outputs=[gr.Textbox(label=\"Answer 1\"), gr.Textbox(label=\"Answer 2\"), gr.Textbox(label=\"Answer 3\")]\n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tkinter as tk\n",
        "\n",
        "def change_text():\n",
        "    user_question = entry.get()\n",
        "    q_embedding = model.encode(user_question)\n",
        "\n",
        "    cos_sim = util.cos_sim(q_embedding, embeddings)\n",
        "\n",
        "    val = []\n",
        "\n",
        "    for i in range(len(cos_sim[0])):\n",
        "        val.append(cos_sim[0][i].item())\n",
        "\n",
        "    p_answer = answer.copy()\n",
        "    p_question = question.copy()\n",
        "\n",
        "    for i in range(len(val)):\n",
        "        for j in range(i + 1, len(val)):\n",
        "            if (val[i] < val[j]):\n",
        "                temp = val[i]\n",
        "                val[i] = val[j]\n",
        "                val[j] = temp\n",
        "\n",
        "                temp = p_answer[i]\n",
        "                p_answer[i] = p_answer[j]\n",
        "                p_answer[j] = temp\n",
        "                \n",
        "                temp = p_question[i]\n",
        "                p_question[i] = p_question[j]\n",
        "                p_question[j] = temp\n",
        "\n",
        "    if (reranker.compute_score([p_question[0], user_question]) > 0):\n",
        "        answer_label.config(text=p_answer[0])\n",
        "    else:\n",
        "        answer_label.config(text=\"Sorry, we don't know the answer for this question\")\n",
        "\n",
        "\n",
        "window = tk.Tk()\n",
        "window.geometry('520x300')\n",
        "label = tk.Label(text=\"FAQ\")\n",
        "label.pack()\n",
        "\n",
        "entry = tk.Entry(fg=\"black\", bg=\"white\", width=50)\n",
        "entry.pack()\n",
        "\n",
        "# Create a button\n",
        "button = tk.Button(window, text=\"Find\", command=change_text)\n",
        "button.pack()\n",
        "\n",
        "noti_label = tk.Label(text=\"Answer\")\n",
        "noti_label.pack()\n",
        "\n",
        "answer_label = tk.Label(text=\"\", wraplength=300, justify='center')\n",
        "answer_label.pack()\n",
        "\n",
        "# Run the main event loop\n",
        "window.mainloop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
